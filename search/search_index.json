{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Best Agentic Harness","text":"<p>A Spring AI advisor that implements LLM-as-a-Judge evaluation with automatic retry.</p> <p></p>"},{"location":"#what-is-best-agentic-harness","title":"What is Best Agentic Harness?","text":"<p>Best Agentic Harness provides a Spring AI advisor \u2014 <code>BestAgenticHarnessAdvisor</code> \u2014 that evaluates LLM responses using point-wise scoring and automatically retries failed attempts with feedback.</p> <p>Inspired by this LinkedIn post on agentic evaluation patterns.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> LLM-as-a-Judge \u2014 Uses a separate LLM to evaluate response quality on a 1\u20134 scale</li> <li> Automatic Retry \u2014 Retries with evaluation feedback appended to the prompt when the rating is below threshold</li> <li> Spring AI Native \u2014 Integrates as a standard Spring AI advisor via <code>ChatClient</code></li> </ul>"},{"location":"#get-started","title":"Get Started","text":"<ul> <li> <p> Quickstart</p> <p>Add the advisor to your project and start evaluating responses.</p> <p> Get started</p> </li> <li> <p> How It Works</p> <p>Understand the evaluation and retry flow in detail.</p> <p> Learn more</p> </li> <li> <p> Configuration</p> <p>Explore all available configuration options.</p> <p> Configure</p> </li> <li> <p> Demo Advisors</p> <p>Learn about the included demo advisors for testing.</p> <p> Explore</p> </li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Java 17+</li> <li>Spring Boot 4.0+</li> <li>Spring AI 2.0.0-M2+</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>All available configuration options for <code>BestAgenticHarnessAdvisor</code>.</p>"},{"location":"configuration/#configuration-options","title":"Configuration Options","text":"<p>The <code>BestAgenticHarnessAdvisor</code> is configured via its builder:</p> <pre><code>BestAgenticHarnessAdvisor.builder()\n    .chatClientBuilder(judgeChatClientBuilder)\n    .successRating(4)\n    .maxRepeatAttempts(3)\n    .build()\n</code></pre>"},{"location":"configuration/#options-reference","title":"Options Reference","text":"Option Default Description <code>chatClientBuilder</code> required <code>ChatClient.Builder</code> for the judge LLM that evaluates responses <code>successRating</code> <code>4</code> Minimum rating (1\u20134) for a response to pass evaluation <code>maxRepeatAttempts</code> <code>3</code> Maximum number of retries before returning the best available response <code>promptTemplate</code> built-in Custom evaluation prompt template for the judge <code>skipEvaluationPredicate</code> skip tool calls Predicate that determines when to skip evaluation entirely"},{"location":"configuration/#details","title":"Details","text":""},{"location":"configuration/#chatclientbuilder","title":"chatClientBuilder","text":"<p>The judge LLM used to evaluate responses. This can be the same model as your primary LLM or a different one. Using a separate, potentially smaller model can reduce costs while still providing good evaluation quality.</p>"},{"location":"configuration/#successrating","title":"successRating","text":"<p>Controls how strict the evaluation is. Setting this to <code>3</code> instead of <code>4</code> will accept \"mostly correct\" responses without retrying for perfection.</p>"},{"location":"configuration/#maxrepeatattempts","title":"maxRepeatAttempts","text":"<p>Limits the number of retries to prevent infinite loops. After exhausting all attempts, the advisor returns the last response regardless of its rating.</p>"},{"location":"configuration/#skipevaluationpredicate","title":"skipEvaluationPredicate","text":"<p>By default, the advisor skips evaluation when the response contains tool calls (since those are intermediate steps). You can provide a custom predicate to control this behavior.</p>"},{"location":"demo-advisors/","title":"Demo Advisors","text":"<p>Included advisors for testing and demonstration.</p>"},{"location":"demo-advisors/#overview","title":"Overview","text":"<p>The following advisors are included for demonstration purposes only. They help test and showcase the <code>BestAgenticHarnessAdvisor</code> evaluation harness.</p>"},{"location":"demo-advisors/#chaosresponseadvisor","title":"ChaosResponseAdvisor","text":"<p>Randomly corrupts LLM responses to test the evaluation harness. This advisor intentionally produces bad responses so you can observe how the judge detects and retries them.</p> <p>Warning</p> <p>This advisor is for testing only. Do not use it in production.</p>"},{"location":"demo-advisors/#myloggingadvisor","title":"MyLoggingAdvisor","text":"<p>A simple request/response logger that prints the conversation messages to the console. Useful for observing the evaluation and retry flow in action.</p>"},{"location":"how-it-works/","title":"How It Works","text":"<p>Understand the LLM-as-a-Judge evaluation and retry flow.</p>"},{"location":"how-it-works/#overview","title":"Overview","text":"<p>The <code>BestAgenticHarnessAdvisor</code> acts as a Spring AI advisor that sits between your application and the LLM. It intercepts responses before they reach the caller and evaluates their quality using a separate judge LLM.</p>"},{"location":"how-it-works/#evaluation-flow","title":"Evaluation Flow","text":"<ol> <li>Intercept the response \u2014 The advisor intercepts the LLM response before it is returned to the caller.</li> <li>Evaluate with the judge \u2014 A separate inner <code>ChatClient</code> (the judge) evaluates the response quality on a 1\u20134 scale.</li> <li>Check the rating \u2014 If the rating meets or exceeds the configured <code>successRating</code> threshold, the response is returned as-is.</li> <li>Retry with feedback \u2014 If the rating is below threshold, the evaluation feedback is appended to the original prompt and the request is retried.</li> <li>Repeat or return \u2014 The process continues until the response passes evaluation or <code>maxRepeatAttempts</code> is reached.</li> </ol>"},{"location":"how-it-works/#evaluation-criteria","title":"Evaluation Criteria","text":"<p>The judge evaluates whether the assistant:</p> <ul> <li>Actually completed the requested task \u2014 Did it answer what was asked?</li> <li>Skipped or faked the response \u2014 Did it give a placeholder or irrelevant answer?</li> <li>Addressed specific requirements \u2014 Did it cover all parts of the request?</li> </ul>"},{"location":"how-it-works/#rating-scale","title":"Rating Scale","text":"Rating Meaning 1 Response is completely wrong or irrelevant 2 Response partially addresses the question 3 Response is mostly correct but incomplete 4 Response fully and correctly addresses the question <p>Note</p> <p>By default, the advisor requires a rating of 4 to pass. You can lower this threshold via the <code>successRating</code> configuration option.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Add the <code>BestAgenticHarnessAdvisor</code> to your Spring AI project.</p>"},{"location":"quickstart/#usage","title":"Usage","text":"<p>Add the <code>BestAgenticHarnessAdvisor</code> to your <code>ChatClient</code> as an advisor. It requires a separate <code>ChatClient.Builder</code> for the judge LLM.</p> <pre><code>ChatClient chatClient = chatClientBuilder\n    .defaultAdvisors(\n        BestAgenticHarnessAdvisor.builder()\n            .chatClientBuilder(judgeChatClientBuilder)\n            .successRating(4)        // 1-4 scale, default: 4\n            .maxRepeatAttempts(3)    // default: 3\n            .build())\n    .build();\n\nvar answer = chatClient.prompt(\"What is the capital of Bulgaria?\").call().content();\n</code></pre>"},{"location":"quickstart/#example-output","title":"Example Output","text":"<p>The advisor intercepts LLM responses, evaluates them, and retries if the quality is below threshold. Here is an example showing a failed first attempt followed by a corrected retry:</p> <pre><code>USER:\n - {\"messageType\":\"USER\",\"metadata\":{\"messageType\":\"USER\"},\n    \"media\":[],\"text\":\"What is the capital of Bulgaria?\"}\n\nASSISTANT:\n - {\"messageType\":\"ASSISTANT\",\"metadata\":{\"messageType\":\"ASSISTANT\"},\n    \"toolCalls\":[],\"media\":[],\n    \"text\":\"The answer is definitely 42.\"}\n\nUSER:\n - {\"messageType\":\"USER\",\"metadata\":{\"messageType\":\"USER\"},\n    \"media\":[],\"text\":\"What is the capital of Bulgaria?\\n\\n\n    EVALUATION FEEDBACK - Your previous response was flagged for\n    not fully completing the task:\\nAnswer the actual question asked.\n    The capital of Bulgaria is Sofia. Provide this factual information\n    directly instead of responding with unrelated references or jokes.\n    \\n\\nIMPORTANT: Address the specific feedback above.\n    Do NOT start over from scratch or delete existing work.\n    Make incremental corrections to actually complete what was\n    originally asked.\\n\"}\n\nASSISTANT:\n - {\"messageType\":\"ASSISTANT\",\"metadata\":{\"messageType\":\"ASSISTANT\"},\n    \"toolCalls\":[],\"media\":[],\n    \"text\":\"Sofia is the capital of Bulgaria.\"}\n</code></pre> <p>In this example:</p> <ol> <li>The first response (\"The answer is definitely 42.\") was evaluated and flagged as incorrect</li> <li>The advisor appended evaluation feedback to the prompt and retried</li> <li>The second response (\"Sofia is the capital of Bulgaria.\") passed evaluation</li> </ol>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li> <p> How It Works</p> <p>Understand the full evaluation and retry flow.</p> <p> Learn more</p> </li> <li> <p> Configuration</p> <p>Tune the advisor to your needs.</p> <p> Configure</p> </li> </ul>"}]}